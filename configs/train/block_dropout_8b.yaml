# Standard training config for Llama-3.1-8B
model:
  name_or_path: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16

press:
  press_type: block_dropout
  block_size: 64
  drop_ratio: 0.3
  protect_start: 4
  protect_recent: 64

curriculum:
  use_curriculum: true
  curriculum_type: linear
  curriculum_start_ratio: 0.0
  curriculum_end_ratio: 0.5
  curriculum_warmup_steps: 1000

training:
  output_dir: ./output/block_dropout_8b
  learning_rate: 1e-5
  num_train_epochs: 1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  max_seq_length: 4096
  warmup_steps: 200
  weight_decay: 0.01
  logging_steps: 10
  save_steps: 500
  save_total_limit: 3
  dataset_name: allenai/c4
  streaming: true
  report_to: tensorboard

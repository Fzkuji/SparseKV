# Adaptive Block Dropout â€” attention-density + sink-aware
model:
  name_or_path: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16

press:
  press_type: adaptive_block_dropout
  block_size: 64
  drop_ratio: 0.3
  protect_start: 4
  protect_recent: 64
  sink_weight: 0.3
  temperature: 1.0

  # Optional sparsity regularization
  sparse_reg_weight: 0.01
  sparse_reg_type: entropy

curriculum:
  use_curriculum: true
  curriculum_type: cosine
  curriculum_start_ratio: 0.0
  curriculum_end_ratio: 0.5
  curriculum_warmup_steps: 1000

training:
  output_dir: ./output/adaptive_dropout_8b
  learning_rate: 1e-5
  num_train_epochs: 1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  max_seq_length: 4096
  warmup_steps: 200
  weight_decay: 0.01
  logging_steps: 10
  save_steps: 500
  output_attentions: true  # Required for adaptive dropout
  dataset_name: allenai/c4
  streaming: true
  report_to: tensorboard
